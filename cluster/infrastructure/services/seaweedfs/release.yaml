# ==============================================================================
# SeaweedFS Helm Release
# ==============================================================================
# Distributed object storage with S3 API
# Uses node labels for placement (no hardcoded hostnames)
# ==============================================================================

apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: seaweedfs
  namespace: seaweedfs
spec:
  interval: 30m
  timeout: 10m
  chart:
    spec:
      chart: seaweedfs
      version: "3.x"
      sourceRef:
        kind: HelmRepository
        name: seaweedfs
        namespace: flux-system

  install:
    remediation:
      retries: 3
  upgrade:
    remediation:
      retries: 3

  values:
    # -------------------------------------------------------------------------
    # Master - manages topology, runs on gpa-server (stable always-on)
    # defaultReplication "001" = replicate once in same rack (both home nodes)
    # -------------------------------------------------------------------------
    master:
      enabled: true
      replicas: 1
      defaultReplication: "001"
      nodeSelector: |
        node.h-kube.io/role: server
      data:
        type: persistentVolumeClaim
        size: 1Gi
        storageClass: local-path

    # -------------------------------------------------------------------------
    # Volume - stores actual data on BOTH home nodes for redundancy
    # Anti-affinity ensures one volume per node (gpa-server + monkeybusiness)
    # -------------------------------------------------------------------------
    volume:
      enabled: true
      replicas: 2
      nodeSelector: |
        node.h-kube.io/location: home
      affinity: |
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchLabels:
                  app.kubernetes.io/component: volume
              topologyKey: kubernetes.io/hostname
      dataDirs:
        - name: data1
          type: "persistentVolumeClaim"
          size: "800Gi"
          storageClass: "local-path"
          maxVolumes: 0
      rack: home
      dataCenter: default

    # -------------------------------------------------------------------------
    # Filer - S3 API + metadata, runs at home (can be on either node)
    # -------------------------------------------------------------------------
    filer:
      enabled: true
      replicas: 1
      nodeSelector: |
        node.h-kube.io/role: server
      s3:
        enabled: true
        port: 8333
        allowEmptyFolder: true
      data:
        type: persistentVolumeClaim
        size: 2Gi
        storageClass: local-path

    # -------------------------------------------------------------------------
    # S3 Gateway - runs at home (can be on either node)
    # Authentication required for Airbyte S3 destination (sends signed requests)
    # -------------------------------------------------------------------------
    s3:
      enabled: true
      enableAuth: true
      existingConfigSecret: seaweedfs-s3-config
      nodeSelector: |
        node.h-kube.io/role: server

    # -------------------------------------------------------------------------
    # Disable unused components
    # -------------------------------------------------------------------------
    # Note: CSI driver is deployed separately via csi-release.yaml
    cronjob:
      enabled: false
    cosi:
      enabled: false
